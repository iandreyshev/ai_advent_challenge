#!/usr/bin/env python3
"""
–ë–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π LLM.

–ò–∑–º–µ—Ä—è–µ—Ç:
- –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
- –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç—Ç–∞–ª–æ–Ω–æ–º)
- –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
- –í–ª–∏—è–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç

–ó–∞–ø—É—Å–∫:
    python3 benchmark.py [–º–æ–¥–µ–ª—å]
"""

import json
import re
import time
import urllib.request
from dataclasses import dataclass
from typing import Any

from prompts import (
    BASIC_SYSTEM, OPTIMIZED_SYSTEM,
    EXTRACTION_BASIC, EXTRACTION_STRUCTURED, EXTRACTION_COT,
    PARAM_CONFIGS
)


OLLAMA_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "qwen2.5"


@dataclass
class BenchmarkResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞."""
    config_name: str
    prompt_type: str
    response_time: float
    response_length: int
    extracted_fields: int
    json_valid: bool
    consistency_score: float


# –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
TEST_CASES = [
    {
        "text": """iPhone 15 Pro Max 256GB –≤ —Ü–≤–µ—Ç–µ —Ç–∏—Ç–∞–Ω–æ–≤—ã–π —á—ë—Ä–Ω—ã–π.
        –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä A17 Pro, –∫–∞–º–µ—Ä–∞ 48 –ú–ø, —ç–∫—Ä–∞–Ω 6.7" Super Retina XDR.
        –¶–µ–Ω–∞: 149 990 —Ä—É–±. –í –Ω–∞–ª–∏—á–∏–∏ –Ω–∞ —Å–∫–ª–∞–¥–µ.""",
        "expected": {
            "–Ω–∞–∑–≤–∞–Ω–∏–µ": "iPhone 15 Pro Max 256GB",
            "—Ü–µ–Ω–∞": "149 990 —Ä—É–±.",
            "–Ω–∞–ª–∏—á–∏–µ": "–≤ –Ω–∞–ª–∏—á–∏–∏"
        }
    },
    {
        "text": """–ü—Ä–æ–¥–∞—é MacBook Air M2 2023 –≥–æ–¥–∞.
        8GB RAM, 512GB SSD. –¶–≤–µ—Ç: —Å–µ—Ä–µ–±—Ä–∏—Å—Ç—ã–π.
        –ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ –∑–∞—Ä—è–¥–∫–∞.
        –¶–µ–Ω–∞ 95000‚ÇΩ, –≤–æ–∑–º–æ–∂–µ–Ω —Ç–æ—Ä–≥.""",
        "expected": {
            "–Ω–∞–∑–≤–∞–Ω–∏–µ": "MacBook Air M2",
            "—Ü–µ–Ω–∞": "95000",
            "–Ω–∞–ª–∏—á–∏–µ": "–≤ –Ω–∞–ª–∏—á–∏–∏"
        }
    },
    {
        "text": """–ö—Ä–æ—Å—Å–æ–≤–∫–∏ Adidas Ultraboost, —Ä–∞–∑–º–µ—Ä 43.
        –ë/—É, –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏. –ü—Ä–æ–±–µ–≥ –æ–∫–æ–ª–æ 100 –∫–º.
        –û—Ç–¥–∞–º –∑–∞ 3500.""",
        "expected": {
            "–Ω–∞–∑–≤–∞–Ω–∏–µ": "Adidas Ultraboost",
            "—Ü–µ–Ω–∞": "3500",
            "–∫–∞—Ç–µ–≥–æ—Ä–∏—è": "–æ–±—É–≤—å"
        }
    }
]


def query_ollama(
    prompt: str,
    model: str = DEFAULT_MODEL,
    system: str = "",
    options: dict | None = None
) -> tuple[str, float]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Ollama."""
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": options or {}
    }

    if system:
        payload["system"] = system

    data = json.dumps(payload).encode("utf-8")
    req = urllib.request.Request(
        OLLAMA_URL,
        data=data,
        headers={"Content-Type": "application/json"}
    )

    start = time.time()
    try:
        with urllib.request.urlopen(req, timeout=120) as response:
            result = json.loads(response.read().decode("utf-8"))
            return result.get("response", ""), time.time() - start
    except Exception as e:
        return f"ERROR: {e}", 0.0


def extract_json_from_response(response: str) -> dict | None:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏."""
    # –ò—â–µ–º JSON –≤ —Ç–µ–∫—Å—Ç–µ
    json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group())
        except json.JSONDecodeError:
            pass

    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON —Å –≤–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏
    json_match = re.search(r'\{.*\}', response, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group())
        except json.JSONDecodeError:
            pass

    return None


def calculate_extraction_score(extracted: dict | None, expected: dict) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (0.0 - 1.0)."""
    if not extracted:
        return 0.0

    matches = 0
    total = len(expected)

    for key, expected_value in expected.items():
        if key in extracted:
            extracted_value = str(extracted[key]).lower()
            expected_lower = str(expected_value).lower()

            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if expected_lower in extracted_value or extracted_value in expected_lower:
                matches += 1
            # –ß–∏—Å–ª–æ–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–¥–ª—è —Ü–µ–Ω)
            elif re.search(r'\d+', expected_lower):
                expected_nums = re.findall(r'\d+', expected_lower)
                extracted_nums = re.findall(r'\d+', extracted_value)
                if set(expected_nums) & set(extracted_nums):
                    matches += 0.5

    return matches / total if total > 0 else 0.0


def run_consistency_test(
    prompt: str,
    model: str,
    system: str,
    options: dict,
    runs: int = 3
) -> tuple[list[str], float]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö."""
    responses = []

    for _ in range(runs):
        response, _ = query_ollama(prompt, model, system, options)
        responses.append(response)

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (–Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ—Ö–æ–∂–∏ –æ—Ç–≤–µ—Ç—ã)
    if len(responses) < 2:
        return responses, 1.0

    # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: –¥–æ–ª—è –æ–±—â–∏—Ö —Å–ª–æ–≤
    def get_words(text: str) -> set:
        return set(re.findall(r'\w+', text.lower()))

    total_similarity = 0
    comparisons = 0

    for i in range(len(responses)):
        for j in range(i + 1, len(responses)):
            words1 = get_words(responses[i])
            words2 = get_words(responses[j])
            if words1 or words2:
                intersection = len(words1 & words2)
                union = len(words1 | words2)
                total_similarity += intersection / union if union > 0 else 0
            comparisons += 1

    consistency = total_similarity / comparisons if comparisons > 0 else 1.0
    return responses, consistency


def run_benchmark(model: str = DEFAULT_MODEL) -> list[BenchmarkResult]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫."""
    results = []

    prompt_configs = [
        ("basic", EXTRACTION_BASIC, BASIC_SYSTEM),
        ("structured", EXTRACTION_STRUCTURED, OPTIMIZED_SYSTEM),
        ("cot", EXTRACTION_COT, OPTIMIZED_SYSTEM),
    ]

    param_configs = [
        ("precise", {"temperature": 0.1, "top_p": 0.5, "num_predict": 300}),
        ("balanced", {"temperature": 0.5, "top_p": 0.9, "num_predict": 300}),
        ("creative", {"temperature": 0.9, "top_p": 0.95, "num_predict": 300}),
    ]

    print("\n" + "=" * 70)
    print(" –ó–ê–ü–£–°–ö –ë–ï–ù–ß–ú–ê–†–ö–ê")
    print("=" * 70)

    total_tests = len(TEST_CASES) * len(prompt_configs) * len(param_configs)
    current = 0

    for test_case in TEST_CASES:
        text = test_case["text"]
        expected = test_case["expected"]

        for prompt_name, prompt_template, system in prompt_configs:
            prompt = prompt_template.format(text=text)

            for config_name, options in param_configs:
                current += 1
                print(f"\r[{current}/{total_tests}] {prompt_name} + {config_name}...", end="", flush=True)

                # –ó–∞–ø—Ä–æ—Å
                response, elapsed = query_ollama(prompt, model, system, options)

                # –ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞
                extracted_json = extract_json_from_response(response)
                extraction_score = calculate_extraction_score(extracted_json, expected)

                # –¢–µ—Å—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è precise –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
                consistency = 1.0
                if config_name == "precise":
                    _, consistency = run_consistency_test(prompt, model, system, options, runs=2)

                result = BenchmarkResult(
                    config_name=config_name,
                    prompt_type=prompt_name,
                    response_time=elapsed,
                    response_length=len(response),
                    extracted_fields=len(extracted_json) if extracted_json else 0,
                    json_valid=extracted_json is not None,
                    consistency_score=consistency
                )
                results.append(result)

    print("\n")
    return results


def print_benchmark_summary(results: list[BenchmarkResult]) -> None:
    """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    print("\n" + "=" * 70)
    print(" –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ù–ß–ú–ê–†–ö–ê")
    print("=" * 70)

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø—É –ø—Ä–æ–º–ø—Ç–∞
    by_prompt = {}
    for r in results:
        if r.prompt_type not in by_prompt:
            by_prompt[r.prompt_type] = []
        by_prompt[r.prompt_type].append(r)

    print("\nüìä –ü–û –¢–ò–ü–£ –ü–†–û–ú–ü–¢–ê:")
    print("-" * 50)

    for prompt_type, prompt_results in by_prompt.items():
        avg_time = sum(r.response_time for r in prompt_results) / len(prompt_results)
        json_rate = sum(1 for r in prompt_results if r.json_valid) / len(prompt_results) * 100
        avg_fields = sum(r.extracted_fields for r in prompt_results) / len(prompt_results)

        print(f"\n  {prompt_type.upper()}:")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}—Å")
        print(f"    –í–∞–ª–∏–¥–Ω—ã–π JSON: {json_rate:.0f}%")
        print(f"    –°—Ä. –ø–æ–ª–µ–π –≤ –æ—Ç–≤–µ—Ç–µ: {avg_fields:.1f}")

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    by_config = {}
    for r in results:
        if r.config_name not in by_config:
            by_config[r.config_name] = []
        by_config[r.config_name].append(r)

    print("\n\nüìä –ü–û –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –ü–ê–†–ê–ú–ï–¢–†–û–í:")
    print("-" * 50)

    for config_name, config_results in by_config.items():
        avg_time = sum(r.response_time for r in config_results) / len(config_results)
        json_rate = sum(1 for r in config_results if r.json_valid) / len(config_results) * 100
        avg_consistency = sum(r.consistency_score for r in config_results) / len(config_results)

        print(f"\n  {config_name.upper()}:")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.2f}—Å")
        print(f"    –í–∞–ª–∏–¥–Ω—ã–π JSON: {json_rate:.0f}%")
        print(f"    –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {avg_consistency:.0%}")

    # –õ—É—á—à–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
    print("\n\nüèÜ –õ–£–ß–®–ò–ï –ö–û–ú–ë–ò–ù–ê–¶–ò–ò:")
    print("-" * 50)

    # –õ—É—á—à–∞—è –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏
    fastest = min(results, key=lambda r: r.response_time)
    print(f"\n  –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π: {fastest.prompt_type} + {fastest.config_name} ({fastest.response_time:.2f}—Å)")

    # –õ—É—á—à–∞—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É JSON
    valid_results = [r for r in results if r.json_valid]
    if valid_results:
        best_quality = max(valid_results, key=lambda r: r.extracted_fields)
        print(f"  –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {best_quality.prompt_type} + {best_quality.config_name} ({best_quality.extracted_fields} –ø–æ–ª–µ–π)")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print("-" * 50)

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    structured_results = [r for r in results if r.prompt_type == "structured"]
    precise_results = [r for r in results if r.config_name == "precise"]

    if structured_results:
        struct_json_rate = sum(1 for r in structured_results if r.json_valid) / len(structured_results)
        if struct_json_rate > 0.7:
            print("\n  ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–∞—é—Ç –±–æ–ª–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π JSON")

    if precise_results:
        precise_consistency = sum(r.consistency_score for r in precise_results) / len(precise_results)
        if precise_consistency > 0.8:
            print("  ‚úÖ –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (precise) –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã")

    print("\n  üìå –î–ª—è –∑–∞–¥–∞—á –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
    print("     - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã —Å –ø—Ä–∏–º–µ—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–∞")
    print("     - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å temperature=0.1-0.3 –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏")
    print("     - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Chain-of-Thought –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    import sys

    model = sys.argv[1] if len(sys.argv) > 1 else DEFAULT_MODEL

    print(f"üî¨ –ë–µ–Ω—á–º–∞—Ä–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ LLM")
    print(f"   –ú–æ–¥–µ–ª—å: {model}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama
    response, _ = query_ollama("test", model, options={"num_predict": 1})
    if "ERROR" in response:
        print(f"\n‚ùå {response}")
        print("\n–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞: ollama serve")
        return

    results = run_benchmark(model)
    print_benchmark_summary(results)

    print("\n" + "=" * 70)
    print(" –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à—ë–Ω")
    print("=" * 70)


if __name__ == "__main__":
    main()
