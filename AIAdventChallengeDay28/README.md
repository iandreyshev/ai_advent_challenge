# День 28: Оптимизация и адаптация локальной LLM

Демонстрация влияния параметров модели и prompt-шаблонов на качество ответов локальной LLM (Ollama).

## Требования

```bash
# Установка Ollama (macOS)
brew install ollama

# Загрузка модели
ollama pull qwen2.5

# Запуск сервера
ollama serve
```

## Запуск

### Интерактивная демонстрация
```bash
python3 optimize_llm.py
```

### Режимы запуска
```bash
python3 optimize_llm.py demo      # Интерактивное меню (по умолчанию)
python3 optimize_llm.py compare   # Сравнение конфигураций параметров
python3 optimize_llm.py prompts   # Сравнение prompt-шаблонов
python3 optimize_llm.py all       # Все демонстрации подряд
```

### Бенчмарк
```bash
python3 benchmark.py              # Полный бенчмарк с метриками
python3 benchmark.py llama3.2     # Использовать другую модель
```

## Параметры оптимизации

### Temperature (креативность)
- `0.0` — детерминированный режим, всегда одинаковый ответ
- `0.3` — низкая креативность, точные ответы
- `0.7` — баланс между точностью и разнообразием
- `1.0` — высокая креативность, разнообразные ответы

### Top-P (nucleus sampling)
- `0.5` — выбор из наиболее вероятных токенов
- `0.9` — более широкий выбор
- `1.0` — рассмотрение всех токенов

### Num_ctx (контекстное окно)
- `1024` — быстро, меньше памяти, короткие тексты
- `2048` — баланс (по умолчанию для большинства моделей)
- `4096+` — для длинных документов

### Num_predict (max_tokens)
- Ограничивает максимальную длину ответа
- Влияет на скорость генерации

## Структура prompt-шаблонов

### Базовый промпт
```
Извлеки данные из текста.
```

### Структурированный промпт
```
Проанализируй текст и верни JSON:
{
    "поле1": "...",
    "поле2": "..."
}
```

### Chain-of-Thought промпт
```
Выполни анализ пошагово:
Шаг 1: ...
Шаг 2: ...
После анализа верни JSON.
```

## Результаты сравнения

| Конфигурация | Скорость | Точность JSON | Консистентность |
|--------------|----------|---------------|-----------------|
| precise (t=0.1) | Средняя | Высокая | Высокая |
| balanced (t=0.5) | Средняя | Средняя | Средняя |
| creative (t=0.9) | Средняя | Низкая | Низкая |

| Тип промпта | Качество | Предсказуемость |
|-------------|----------|-----------------|
| Базовый | Низкое | Низкая |
| Структурированный | Высокое | Высокая |
| Chain-of-Thought | Высокое | Средняя |

## Рекомендации

**Для извлечения данных:**
- Temperature: 0.1-0.3
- Структурированный промпт с примером формата
- Детальный системный промпт с ограничениями

**Для генерации текста:**
- Temperature: 0.7-0.9
- Менее жёсткие ограничения
- Больший num_predict

**Для классификации:**
- Temperature: 0.0-0.2
- Чёткие категории в промпте
- JSON-формат для ответа

## Файлы

- `optimize_llm.py` — основной скрипт демонстрации
- `benchmark.py` — бенчмарк с метриками
- `prompts.py` — коллекция prompt-шаблонов и конфигураций
