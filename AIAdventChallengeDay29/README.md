# День 29: Локальный аналитик данных

Python-приложение для анализа данных через локальную LLM (Ollama).

## Возможности

- Загрузка CSV и JSON файлов
- Автоматическая статистика и частотный анализ
- Интерактивные вопросы об данных
- Стриминг ответов в реальном времени

## Использование

```bash
python3 analyst.py <путь_к_файлу> [модель]

# Примеры
python3 analyst.py sample_data/server_logs.csv
python3 analyst.py sample_data/user_funnel.json qwen2.5
```

## Технологии

- **Python 3.8+** — основной язык
- **Ollama** — локальная LLM
- **urllib** — HTTP запросы
- **csv/json** — парсинг данных

## Структура

```
AIAdventChallengeDay29/
├── analyst.py       # Точка входа, интерактивный цикл
├── data_loader.py   # Загрузка CSV/JSON
├── prompts.py       # Системные промпты
└── sample_data/
    ├── server_logs.csv
    └── user_funnel.json
```

## Параметры LLM

| Параметр | Значение |
|----------|----------|
| temperature | 0.2 |
| num_ctx | 8192 |
| model | llama3 (по умолчанию) |

## Примеры вопросов

- "Какие ошибки чаще всего встречаются?"
- "Покажи статистику по HTTP статусам"
- "Какой средний response time?"
- "Сколько пользователей дошло до оплаты?"

## Команды выхода

- `выход`, `exit`, `quit`, `q`

## Требования

```bash
# Запустить Ollama
ollama serve

# Загрузить модель
ollama pull llama3
```

## Особенности

- Модель отвечает ТОЛЬКО на основе данных (не выдумывает)
- История диалога сохраняется в сессии
- Поддержка любых табличных данных
