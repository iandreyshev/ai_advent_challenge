#!/usr/bin/env python3
"""
–î–µ–Ω—å 31: –ì–æ–ª–æ—Å–æ–≤–æ–π –∞–≥–µ–Ω—Ç —Å –ø—Ä—è–º–æ–π –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∞—É–¥–∏–æ –≤ –º–æ–¥–µ–ª—å

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
1. –ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ –≤ —Ñ–∞–π–ª (WAV)
2. –û—Ç–ø—Ä–∞–≤–∫–∞ –∞—É–¥–∏–æ –≤ –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (Whisper)
3. –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
4. –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ LLM
5. –í—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
"""

import argparse
import sys
import os
import wave
import tempfile
from typing import Optional
import pyaudio
import requests


class DirectVoiceAgent:
    """–ì–æ–ª–æ—Å–æ–≤–æ–π –∞–≥–µ–Ω—Ç —Å –ø—Ä—è–º–æ–π –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∞—É–¥–∏–æ –≤ –º–æ–¥–µ–ª—å"""

    def __init__(
        self,
        llm_model: str = "qwen2.5",
        whisper_mode: str = "api",  # "api" –∏–ª–∏ "local"
        host: str = "localhost",
        port: int = 11434,
        sample_rate: int = 16000,
        chunk_size: int = 1024,
        record_seconds: int = 5
    ):
        self.llm_model = llm_model
        self.whisper_mode = whisper_mode
        self.ollama_url = f"http://{host}:{port}/api/generate"
        self.sample_rate = sample_rate
        self.chunk_size = chunk_size
        self.record_seconds = record_seconds

        # Whisper API –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if whisper_mode == "api" and not self.openai_api_key:
            print("‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --mode local")

        # PyAudio
        self.audio = pyaudio.PyAudio()

    def record_audio(self) -> Optional[str]:
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∞—É–¥–∏–æ —Å –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª"""
        print("üé§ –ì–æ–≤–æ—Ä–∏—Ç–µ —Å–µ–π—á–∞—Å...")

        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫ –¥–ª—è –∑–∞–ø–∏—Å–∏
            stream = self.audio.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=self.sample_rate,
                input=True,
                frames_per_buffer=self.chunk_size
            )

            frames = []

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∞—É–¥–∏–æ
            for i in range(0, int(self.sample_rate / self.chunk_size * self.record_seconds)):
                data = stream.read(self.chunk_size)
                frames.append(data)

                # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–ø–∏—Å–∏
                if i % 10 == 0:
                    print(".", end="", flush=True)

            print("\n‚úÖ –ó–∞–ø–∏—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

            # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫
            stream.stop_stream()
            stream.close()

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            temp_filename = temp_file.name

            with wave.open(temp_filename, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(self.audio.get_sample_size(pyaudio.paInt16))
                wf.setframerate(self.sample_rate)
                wf.writeframes(b''.join(frames))

            return temp_filename

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏: {e}")
            return None

    def transcribe_audio_api(self, audio_file: str) -> Optional[str]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ –≤ OpenAI Whisper API –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        print("üîÑ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Whisper API...")

        try:
            url = "https://api.openai.com/v1/audio/transcriptions"
            headers = {
                "Authorization": f"Bearer {self.openai_api_key}"
            }

            with open(audio_file, "rb") as f:
                files = {
                    "file": (os.path.basename(audio_file), f, "audio/wav"),
                    "model": (None, "whisper-1"),
                    "language": (None, "ru")
                }

                response = requests.post(url, headers=headers, files=files, timeout=30)
                response.raise_for_status()

                result = response.json()
                return result.get("text", "").strip()

        except requests.exceptions.ConnectionError:
            print("‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ OpenAI API")
            return None
        except requests.exceptions.Timeout:
            print("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Whisper API")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏: {e}")
            return None

    def transcribe_audio_local(self, audio_file: str) -> Optional[str]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (—á–µ—Ä–µ–∑ Ollama —Å Whisper)"""
        print("üîÑ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å...")

        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: Ollama –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ –Ω–∞–ø—Ä—è–º—É—é
        # –≠—Ç–æ –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è –±—É–¥—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
        # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π Whisper —á–µ—Ä–µ–∑ whisper-cpp –∏–ª–∏ faster-whisper

        try:
            import whisper

            print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Whisper...")
            model = whisper.load_model("base")

            print("üéØ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...")
            result = model.transcribe(audio_file, language="ru")

            return result.get("text", "").strip()

        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai-whisper –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai-whisper")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏: {e}")
            return None

    def transcribe_audio(self, audio_file: str) -> Optional[str]:
        """–†–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –∞—É–¥–∏–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º"""
        if self.whisper_mode == "api":
            return self.transcribe_audio_api(audio_file)
        else:
            return self.transcribe_audio_local(audio_file)

    def query_llm(self, text: str) -> str:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –≤ LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç"""
        try:
            payload = {
                "model": self.llm_model,
                "prompt": text,
                "stream": False
            }

            response = requests.post(self.ollama_url, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()
            return result.get("response", "").strip()

        except requests.exceptions.ConnectionError:
            return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama"
        except requests.exceptions.Timeout:
            return "‚ùå –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –∏—Å—Ç–µ–∫–ª–æ"
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞: {e}"

    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –∞–≥–µ–Ω—Ç–∞"""
        print("=" * 60)
        print("üéôÔ∏è  –ì–û–õ–û–°–û–í–û–ô –ê–ì–ï–ù–¢ (Direct Audio ‚Üí Whisper ‚Üí LLM)")
        print("=" * 60)
        print(f"LLM –º–æ–¥–µ–ª—å: {self.llm_model}")
        print(f"Whisper —Ä–µ–∂–∏–º: {self.whisper_mode}")
        print(f"–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏: {self.record_seconds} —Å–µ–∫")
        print("-" * 60)
        print("–ö–æ–º–∞–Ω–¥—ã: —Å–∫–∞–∂–∏—Ç–µ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è")
        print("=" * 60)
        print()

        while True:
            # –®–∞–≥ 1: –ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ
            audio_file = self.record_audio()
            if not audio_file:
                continue

            try:
                # –®–∞–≥ 2: –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
                recognized_text = self.transcribe_audio(audio_file)

                if not recognized_text:
                    print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.")
                    continue

                print(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {recognized_text}")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–∞–Ω–¥—ã –≤—ã—Ö–æ–¥–∞
                if recognized_text.lower() in ["–≤—ã—Ö–æ–¥", "—Å—Ç–æ–ø", "exit", "stop", "quit"]:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    break

                # –®–∞–≥ 3: –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ LLM
                print("ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
                response = self.query_llm(recognized_text)

                # –®–∞–≥ 4: –í—ã–≤–æ–¥ –æ—Ç–≤–µ—Ç–∞
                print("\n" + "=" * 60)
                print("üí¨ –û–¢–í–ï–¢:")
                print("-" * 60)
                print(response)
                print("=" * 60)
                print()

            finally:
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if os.path.exists(audio_file):
                    os.remove(audio_file)

    def __del__(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if hasattr(self, 'audio'):
            self.audio.terminate()


def main():
    parser = argparse.ArgumentParser(
        description="–ì–æ–ª–æ—Å–æ–≤–æ–π –∞–≥–µ–Ω—Ç —Å –ø—Ä—è–º–æ–π –æ—Ç–ø—Ä–∞–≤–∫–æ–π –∞—É–¥–∏–æ –≤ –º–æ–¥–µ–ª—å",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –° OpenAI Whisper API (—Ç—Ä–µ–±—É–µ—Ç OPENAI_API_KEY)
  python3 voice_agent_direct.py --mode api

  # –° –ª–æ–∫–∞–ª—å–Ω—ã–º Whisper (—Ç—Ä–µ–±—É–µ—Ç pip install openai-whisper)
  python3 voice_agent_direct.py --mode local

  # –ò–∑–º–µ–Ω–∏—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏
  python3 voice_agent_direct.py --duration 10

  # –î—Ä—É–≥–∞—è LLM –º–æ–¥–µ–ª—å
  python3 voice_agent_direct.py --model llama3.2

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø—Ä—è–º–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ:
- –ë–æ–ª—å—à–µ –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –ø—Ä–æ—Ü–µ—Å—Å–æ–º –∑–∞–ø–∏—Å–∏
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- –†–∞–±–æ—Ç–∞ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
        """
    )

    parser.add_argument(
        "--model",
        default=os.getenv("OLLAMA_MODEL", "qwen2.5"),
        help="LLM –º–æ–¥–µ–ª—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: qwen2.5)"
    )

    parser.add_argument(
        "--mode",
        choices=["api", "local"],
        default="api",
        help="–†–µ–∂–∏–º Whisper: api (OpenAI) –∏–ª–∏ local (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)"
    )

    parser.add_argument(
        "--host",
        default=os.getenv("OLLAMA_HOST", "localhost"),
        help="–•–æ—Å—Ç Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: localhost)"
    )

    parser.add_argument(
        "--port",
        type=int,
        default=int(os.getenv("OLLAMA_PORT", "11434")),
        help="–ü–æ—Ä—Ç Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 11434)"
    )

    parser.add_argument(
        "--duration",
        type=int,
        default=5,
        help="–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5)"
    )

    parser.add_argument(
        "--sample-rate",
        type=int,
        default=16000,
        help="–ß–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 16000)"
    )

    args = parser.parse_args()

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –∞–≥–µ–Ω—Ç–∞
    agent = DirectVoiceAgent(
        llm_model=args.model,
        whisper_mode=args.mode,
        host=args.host,
        port=args.port,
        sample_rate=args.sample_rate,
        record_seconds=args.duration
    )

    try:
        agent.run()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
        sys.exit(0)


if __name__ == "__main__":
    main()
